{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c814820",
   "metadata": {},
   "source": [
    "# About the notebook\n",
    "The purpose of this Jupyter Notebook is to use a pre-trained deep learning model to generate class predictions for a given input image.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad28b6b",
   "metadata": {},
   "source": [
    "# 00 - Special Instructions for Google Colab Users \n",
    "\n",
    "The following lines of code should be executed only when running your script on Google Colab. This is crucial to leverage the additional features provided by Colab, most notably, the availability of a free GPU.  **If, you're running the code locally, this line can be skipped (GO TO STEP 01 - Loading dependencies) as it pertains specifically to the Colab setup.**\n",
    "\n",
    "## Give access to google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37144f95",
   "metadata": {},
   "source": [
    "## Install Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install napari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7819107",
   "metadata": {},
   "source": [
    "## Copy code to current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/paul-hernandez-herrera/image_classification_pytorch\n",
    "import os\n",
    "workbookDir = \"/content/image_classification_pytorch/\"\n",
    "os.chdir(workbookDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fa4ea",
   "metadata": {},
   "source": [
    "# 01 - Loading dependencies\n",
    "In this notebook, before running any code, there are several libraries and modules that need to be imported to ensure that the notebook runs smoothly. These libraries and modules contain pre-written code that performs specific tasks, such as reading and processing images, defining the UNET model, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18617ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'workbookDir' not in globals():\n",
    "    print('Updating working directory')\n",
    "    workbookDir = os.path.dirname(os.getcwd())\n",
    "    os.chdir(workbookDir)\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from core_code.regression_predict_neck_point_2D import remove_head_from_trace\n",
    "\n",
    "#allow reloading the functions updates\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91655c7c",
   "metadata": {},
   "source": [
    "# 02 - Setting required parameters\n",
    "In this section, users can specify the necessary parameters to predict the segmentation mask for a given input image. The following parameters are required:\n",
    "\n",
    "**Model path**: The path to the trained model that will be used for segmentation prediction.\n",
    "\n",
    "**3D Trace path**: The path to the folder containing the raw traces in format \"swc\".\n",
    "\n",
    "**3D Images path**: The path to the folder containing the 3D images corresponding to the traces.\n",
    "\n",
    "**Device**: The device that will be used to perform the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = remove_head_from_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf4f4d",
   "metadata": {},
   "source": [
    "# 03 - Do the prediction\n",
    "This line of code allows you to predict the images using the trained deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21589787",
   "metadata": {},
   "source": [
    "## 04 - Manually correct erroneus traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950980f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_code.regression_predict_neck_point_2D import manually_remove_head\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337ea8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trace_p = r\"C:\\Users\\jalip\\Documentos\\Proyectos\\Sperm\\Campo_claro\\HIGH_VISCOCITY\\2017_11_22_HIGH_VISCOCITY_DONE\\Exp4_stacks\\Exp4_stacks_TP0312_DC_LogN_rec_FM.swc\"\n",
    "for i in range(21,22):\n",
    "    trace_p = Path(predict.trace_path_w.value, \"Exp8_stacks_TP\" + f\"{i:0{4}}\" +\"_DC_LogN_rec_FM.swc\")\n",
    "    #trace_p = Path(predict.trace_path_w.value, \"Exp17_stacks_TP0049_DC_LogN_rec_FM.swc\")\n",
    "    folder_images = Path(predict.images_path_w.value)\n",
    "    folder_output = Path(Path(predict.trace_path_w.value), \"trace_head_removed\")\n",
    "    trace_p = Path(trace_p)\n",
    "    ind = 10\n",
    "    manually_remove_head(trace_p, folder_images, folder_output, ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fb338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
